{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1sXm2wt519o"
      },
      "source": [
        "# Programming Problem 1: HMM for NER (30 points)\n",
        "Welcome to the programming portion of the assignment! Each assignment throughout the semester will have a written portion and a programming portion. We will be using [Google Colab](https://colab.research.google.com/notebooks/intro.ipynb#recent=true), so if you have never used it before, take a quick look through this introduction: [Working with Google Colab](https://docs.google.com/document/d/1LlnXoOblXwW3YX-0yG_5seTXJsb3kRdMMRYqs8Qqum4/edit?usp=sharing).\n",
        "\n",
        "### Writing Code\n",
        "Look for the keyword \"TODO\" and fill in your code in the empty space.\n",
        "Feel free to add and delete arguments in function signatures, but be careful that you might need to change them in function calls which are already present in the notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fEyk2ch1GYi0"
      },
      "source": [
        "### Data preprocessing\n",
        "\n",
        "In this section we will write code to load data and build the dataset for Named Entity Recognition."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X_hx0PKdII9C",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import codecs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmssMqR2IKkC"
      },
      "source": [
        "Write a function to load sentences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q4MKd94oHDKr",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "def zero_digits(s):\n",
        "    \"\"\"\n",
        "    Replace all digits in a string with zeros.\n",
        "    \"\"\"\n",
        "    return re.sub('\\d', '0', s)\n",
        "\n",
        "def load_sentences(path):\n",
        "    \"\"\"\n",
        "    Load sentences. A line must contain at least a word and its tag.\n",
        "    Sentences are separated by empty lines.\n",
        "    \"\"\"\n",
        "    sentences = [] #Initialize an empty list to store the sentences\n",
        "    sentence = [] # Initialize an empty list to store words in the current sentence\n",
        "    for line in codecs.open(path, 'r', 'utf8'):  # \"codecs.open\": module in Python used for opening files with specific encodings\n",
        "        line = zero_digits(line.rstrip()) # rstrip() method is used to remove any trailing whitespace from the line, and zero_digits replaces digits with zeros\n",
        "        if not line: # the line is empty (not line), donc end of sentence.\n",
        "            if len(sentence) > 0: # donc if the sentence list is not empty\n",
        "                if 'DOCSTART' not in sentence[0][0]: # the first word in the sentence is not 'DOCSTART'\n",
        "                    sentences.append(sentence) # donc the sentence is appended to the sentences list\n",
        "                sentence = [] #  and the sentence list is reset\n",
        "        else: # If the line is not empty\n",
        "            word = line.split() # split the line into words\n",
        "            assert len(word) >= 2 # ensures that each line contains at least a word and its tag\n",
        "            sentence.append(word) # The word and its tag are appended to the sentence list\n",
        "    if len(sentence) > 0:\n",
        "        if 'DOCSTART' not in sentence[0][0]:\n",
        "            sentences.append(sentence) # Append any remaining words in the last sentence to the sentences list\n",
        "    return sentences #list of sentences -> each sentence is a list of word-tag pairs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2W76Clg4J2wo"
      },
      "source": [
        "Prepare the training/test data using the loaded sentences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1yS9JsVSMZ6l",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "def create_dico(item_list): # each sublist in item_list parameter is assumed to have items that need to be counted\n",
        "    \"\"\"\n",
        "    Create a dictionary of items from a list of list of items.\n",
        "    \"\"\"\n",
        "    assert type(item_list) is list #ensures ghat item_list is a list\n",
        "    dico = {} # initialize an emty dictionary to store the count of items\n",
        "    for items in item_list:\n",
        "        for item in items:\n",
        "            if item not in dico: # checks is the item is not repeated in the dicstionary\n",
        "                dico[item] = 1 #new item donc add it's count by 1\n",
        "            else:\n",
        "                dico[item] += 1 #repeated item donc increment it count by 1\n",
        "    return dico #returns a dictionary with the fount of items"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D_7t9tVCMdFY",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "def create_mapping(dico): # -> takes a dictionary dico: keys-> items (words / tags), values -> their counts.\n",
        "    \"\"\"\n",
        "    Create a mapping (item to ID / ID to item) from a dictionary.\n",
        "    Items are ordered by decreasing frequency.\n",
        "    \"\"\"\n",
        "    sorted_items = sorted(dico.items(), key=lambda x: (-x[1], x[0])) # sorts items in decreasing order of frequency\n",
        "    id_to_item = {i: v[0] for i, v in enumerate(sorted_items) if v[1] > 2} # maps: keys as words/tags and values as counts: excluding el counts el olayela the rare / infrequent words\n",
        "    item_to_id = {v: k for k, v in id_to_item.items()} # maps: keys as counts and values as words/tags\n",
        "    return item_to_id, id_to_item\n",
        "\n",
        "def word_mapping(sentences):\n",
        "    \"\"\"\n",
        "    Create a dictionary and a mapping of words, sorted by frequency.\n",
        "    \"\"\"\n",
        "    words = [[x[0] for x in s] for s in sentences] # extract kol words men el sentences\n",
        "    dico = create_dico(words) #-> dictionary of word counts\n",
        "    dico['<UNK>'] = 10000000 # special token that represents unknown words\n",
        "    word_to_id, id_to_word = create_mapping(dico)\n",
        "    print(\"Found %i unique words (%i in total)\" % (\n",
        "        len(dico), sum(len(x) for x in words))\n",
        "    )\n",
        "    return dico, word_to_id, id_to_word\n",
        "\n",
        "def tag_mapping(sentences): # nafs el haga fel tags\n",
        "    \"\"\"\n",
        "    Create a dictionary and a mapping of tags, sorted by frequency.\n",
        "    \"\"\"\n",
        "    tags = [[word[-1] for word in s] for s in sentences]\n",
        "    dico = create_dico(tags)\n",
        "    tag_to_id, id_to_tag = create_mapping(dico)\n",
        "    print(\"Found %i unique named entity tags\" % len(dico))\n",
        "    return dico, tag_to_id, id_to_tag"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lNS57L87IQdT",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "def prepare_dataset(sentences, mode=None, word_to_id=None, tag_to_id=None):\n",
        "    \"\"\"\n",
        "    Prepare the dataset. Return 'data', which is a list of lists of dictionaries containing:\n",
        "        - words (strings)\n",
        "        - word indexes\n",
        "        - tag indexes\n",
        "    \"\"\"\n",
        "    assert mode == 'train' or (mode == 'test' and word_to_id and tag_to_id) # assertion en el mode train aw test mode\n",
        "\n",
        "    if mode=='train':\n",
        "        word_dic, word_to_id, id_to_word = word_mapping(sentences)\n",
        "        tag_dic, tag_to_id, id_to_tag = tag_mapping(sentences)\n",
        "\n",
        "    def f(x): return x\n",
        "    data = []\n",
        "    for s in sentences:  # Iterates over each sentence\n",
        "        str_words = [w[0] for w in s] # Extracts the words and tags from each sentence\n",
        "        words = [word_to_id[f(w) if f(w) in word_to_id else '<UNK>'] # Converts each word ll id w law msh mawgouda -> <UNK>\n",
        "                 for w in str_words]\n",
        "        tags = [tag_to_id[w[-1]] for w in s] # nafs el kalam fel tags\n",
        "        data.append({ #Appends dict (data)\n",
        "            'str_words': str_words, #  the original words\n",
        "            'words': words, # IDs lel words\n",
        "            'tags': tags, # IDs el corresponding tags\n",
        "        })\n",
        "\n",
        "    if mode == 'train':\n",
        "        return data, {'word_to_id':word_to_id, 'id_to_word':id_to_word, 'tag_to_id':tag_to_id, 'id_to_tag':id_to_tag}\n",
        "    else:\n",
        "        return data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LgikvX-qKTL7"
      },
      "source": [
        "### Hidden Markov Model\n",
        "In this section, we will implement a bigram hidden markov model (HMM) that could perform two types of decoding: greedy decoding and viterbi decoding.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "te89fFIgLdFf",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "from sklearn import linear_model\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from scipy import sparse\n",
        "import collections\n",
        "import codecs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QCiZJCYKMln5",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "class HMM(object):\n",
        "    \"\"\"\n",
        "     HMM Model\n",
        "    \"\"\"\n",
        "    def __init__(self, dic, decode_type):\n",
        "        \"\"\"\n",
        "        Initialize the model.\n",
        "        \"\"\"\n",
        "\n",
        "        self.num_words = len(dic['word_to_id'])\n",
        "        self.num_tags = len(dic['tag_to_id'])\n",
        "\n",
        "        self.initial_prob = np.ones([self.num_tags])\n",
        "        self.transition_prob = np.ones([self.num_tags, self.num_tags])\n",
        "        self.emission_prob = np.ones([self.num_tags, self.num_words])\n",
        "        self.decode_type = decode_type\n",
        "\n",
        "        return\n",
        "\n",
        "    def train(self, corpus):\n",
        "        \"\"\"\n",
        "        TODO: Train a bigram HMM model using MLE estimates.\n",
        "        Complete the code to compute self,initial_prob, self.transition_prob and self.emission_prob appropriately.\n",
        "\n",
        "        Args:\n",
        "            corpus is a list of dictionaries of the form:\n",
        "            {'str_words': str_words,   ### List of string words\n",
        "            'words': words,            ### List of word IDs\n",
        "            'tags': tags}              ### List of tag IDs\n",
        "            All three lists above have length equal to the sentence length for each instance.\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        # Step 1: Compute initial_probs.\n",
        "        # initial_prob[x]: the probability of tag x to be assigned to the first token in a sentence.\n",
        "        self.initial_prob = np.zeros([self.num_tags])\n",
        "        for sentence in corpus:\n",
        "            # TODO: update self.initial_prob\n",
        "            # (5 points)\n",
        "            # START HERE\n",
        "            self.initial_prob[sentence['tags'][0]] += 1\n",
        "            # END HERE\n",
        "\n",
        "        # Normarlize initial_prob to sum to 1\n",
        "        self.initial_prob /= np.sum(self.initial_prob)\n",
        "\n",
        "        # Step 2: Complete the code to compute transition_prob.\n",
        "        # transition_prob[x][y]: the probability of tag y to appear after tag x.\n",
        "        self.transition_prob = np.zeros([self.num_tags, self.num_tags])\n",
        "\n",
        "        for sentence in corpus:\n",
        "            tag = sentence['tags']\n",
        "            for i in range(1, len(tag)):\n",
        "                # TODO: update self.transition_prob\n",
        "                # (5 points)\n",
        "                # START HERE\n",
        "                self.transition_prob[tag[i-1], tag[i]] += 1\n",
        "                # END HERE\n",
        "\n",
        "        # Normalize every row of transition_prob to sum to 1.\n",
        "        for p in self.transition_prob:\n",
        "            p /= np.sum(p)\n",
        "\n",
        "        # Step 3: Complete the code to compute emission_prob\n",
        "        # emission_prob[x][y]: the probability of word y to appear given tag x.\n",
        "        # Note that for each sentence s in the corpus, word IDs are in s['words'].\n",
        "        self.emission_prob = np.zeros([self.num_tags, self.num_words])\n",
        "        for sentence in corpus:\n",
        "          #for i in range(len(sentence['tags'])):\n",
        "          for i in range(len(sentence['tags'])):\n",
        "                # TODO: update self.emission_prob\n",
        "                # (5 points)\n",
        "                # START HERE\n",
        "                self.emission_prob[sentence['tags'][i], sentence['words'][i]] += 1\n",
        "                # END HERE\n",
        "\n",
        "        # For every tag, normalize the emission_prob to sum to 1.\n",
        "        for p in self.emission_prob:\n",
        "            p /= np.sum(p)\n",
        "        return\n",
        "\n",
        "    def greedy_decode(self, sentence):\n",
        "        \"\"\"\n",
        "        Decode a single sentence in Greedy fashion\n",
        "        Return a list of tags.\n",
        "        \"\"\"\n",
        "        tags = []\n",
        "\n",
        "        init_scores = [self.initial_prob[t] * self.emission_prob[t][sentence[0]] for t in range(self.num_tags)]\n",
        "        tags.append(np.argmax(init_scores))\n",
        "\n",
        "        for w in sentence[1:]:\n",
        "            scores = [self.transition_prob[tags[-1]][t] * self.emission_prob[t][w] for t in range(self.num_tags)]\n",
        "            tags.append(np.argmax(scores))\n",
        "\n",
        "        assert len(tags) == len(sentence)\n",
        "        return tags\n",
        "\n",
        "    def viterbi_decode(self, sentence):\n",
        "        \"\"\"\n",
        "        TODO: Complete the code to decode a single sentence using the Viterbi algorithm.\n",
        "        Args:\n",
        "             sentence -- a list of ints that represents word IDs.\n",
        "        Output:\n",
        "             tags     -- a list of ints that represents the tags decoded from the input.\n",
        "        \"\"\"\n",
        "        tags = []\n",
        "        l = len(sentence)\n",
        "\n",
        "        pi = self.initial_prob\n",
        "        A = self.transition_prob\n",
        "        O = self.emission_prob\n",
        "\n",
        "        # Let M be the state matrix.\n",
        "        # M[i,j]: most probable sequence of tags ending with tag j at the i-th token.\n",
        "        M = np.zeros((l, self.num_tags))\n",
        "        M[:,:] = float('-inf')\n",
        "\n",
        "        # Use B to track the path to reach the most probable sequence.\n",
        "        # B[i,j] is the tag of the (i-1)-th token in the most probable sequence ending with tag j at the i-th token.\n",
        "        B = np.zeros((l, self.num_tags), 'int')\n",
        "\n",
        "        # Compute the probability to assign each tag to the first token.\n",
        "        M[0, :] = pi * O[:, sentence[0]]\n",
        "\n",
        "        # Dynamic programming.\n",
        "        for i in range(1, l):\n",
        "            for j in range(self.num_tags):\n",
        "                # TODO: Compute M[i, j] and B[i, j].\n",
        "                # (10 points)\n",
        "                # START HERE\n",
        "                scores = M[i-1] + np.log(A[:, j]) + np.log(O[j, sentence[i]])\n",
        "                M[i, j], B[i, j] = np.max(scores), np.argmax(scores)\n",
        "                # END HERE\n",
        "\n",
        "        # Extract the optimal sequence of tags from B.\n",
        "        # Start from the last position, and iteratively find the tag of each position that results in the most probable tag sequence.\n",
        "        tags.append(np.argmax(M[l-1,:]))\n",
        "        for i in range(l-1, 0, -1):\n",
        "            # TODO: Extract the tag of the (i-1)-th token that results in the most probable sequence of tags.\n",
        "            # (5 points)\n",
        "            # START HERE\n",
        "            tags.append(B[i, tags[-1]])\n",
        "            # END HERE\n",
        "        tags.reverse()\n",
        "        return tags\n",
        "\n",
        "    def tag(self, sentence):\n",
        "        \"\"\"\n",
        "        Tag a sentence using a trained HMM.\n",
        "        \"\"\"\n",
        "        if self.decode_type == 'viterbi':\n",
        "            return self.viterbi_decode(sentence)\n",
        "        elif self.decode_type == 'greedy':\n",
        "            return self.greedy_decode(sentence)\n",
        "        else:\n",
        "            raise ValueError(\"Unknown decoding type\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6P0Qs5JM0x3"
      },
      "source": [
        "### Train and evaluate HMMs.\n",
        "This section contains driver code for learning a HMM for named entity recognition on a training corpus and evaluating it on a test corpus."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DJRRLA2jOsl1",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import codecs\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import collections\n",
        "from sklearn.metrics import f1_score, confusion_matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vk2KF59LOzSr"
      },
      "source": [
        "Write a function to tag the test corpus with a trained model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ldGSAHWSOwt1",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "def tag_corpus(model, test_corpus, output_file, dic):\n",
        "    if output_file:\n",
        "        f_output = codecs.open(output_file, 'w', 'utf-8')\n",
        "    start = time.time()\n",
        "\n",
        "    num_correct = 0.\n",
        "    num_total = 0.\n",
        "    y_pred=[]\n",
        "    y_actual=[]\n",
        "    print('Tagging...')\n",
        "    for i, sentence in enumerate(tqdm(test_corpus)):\n",
        "        tags = model.tag(sentence['words'])\n",
        "        str_tags = [dic['id_to_tag'][t] for t in tags]\n",
        "        y_pred.extend(tags)\n",
        "        y_actual.extend(sentence['tags'])\n",
        "\n",
        "        # Check accuracy.\n",
        "        num_correct += np.sum(np.array(tags) == np.array(sentence['tags']))\n",
        "        num_total += len([w for w in sentence['words']])\n",
        "\n",
        "        if output_file:\n",
        "            f_output.write('%s\\n' % ' '.join('%s%s%s' % (w, '__', y)\n",
        "                                             for w, y in zip(sentence['str_words'], str_tags)))\n",
        "\n",
        "    print('---- %i lines tagged in %.4fs ----' % (len(test_corpus), time.time() - start))\n",
        "    if output_file:\n",
        "        f_output.close()\n",
        "\n",
        "    print(\"Overall accuracy: %s\\n\" % (num_correct/num_total))\n",
        "    return y_pred,y_actual"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KC_VhUG0O98M",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "def compute_score(y_pred,y_actual):\n",
        "    A = confusion_matrix(y_actual, y_pred)\n",
        "    f1 = f1_score(y_actual, y_pred,average=None)\n",
        "    print(\"Confusion Matrix:\\n\", A)\n",
        "    print(\"F-1 scores: \", f1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLFBiD4BPMl1"
      },
      "source": [
        "Write a function to train and evalute the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RFcYYThhPRbh",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "def runHiddenMarkovModel(train_corpus,\n",
        "                         test_corpus,\n",
        "                         dic,\n",
        "                         decode_type,\n",
        "                         output_file):\n",
        "    # build and train the model\n",
        "    model = HMM(dic, decode_type)\n",
        "    model.train(train_corpus)\n",
        "\n",
        "    print(\"Train results:\")\n",
        "    pred, real = tag_corpus(model, train_corpus, output_file, dic)\n",
        "\n",
        "    print(\"Tags: \", dic['id_to_tag'])\n",
        "    A = compute_score(pred,real)\n",
        "\n",
        "    # test on validation\n",
        "    print(\"\\n-----------\\nTest results:\")\n",
        "    pred, real = tag_corpus(model, test_corpus, output_file, dic)\n",
        "\n",
        "    print(\"Tags: \", dic['id_to_tag'])\n",
        "    A = compute_score(pred,real)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-vRsc-YrRAb2"
      },
      "source": [
        "### Experiments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "McZ9KpeQRlA9"
      },
      "source": [
        "#### Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mjVwUkR9Rq53",
        "vscode": {
          "languageId": "python"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db457f2d-e66f-4304-e740-25c6fe95351b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-03-21 03:24:02--  https://princeton-nlp.github.io/cos484/assignments/a2/eng.train\n",
            "Resolving princeton-nlp.github.io (princeton-nlp.github.io)... 185.199.111.153, 185.199.109.153, 185.199.110.153, ...\n",
            "Connecting to princeton-nlp.github.io (princeton-nlp.github.io)|185.199.111.153|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3213441 (3.1M) [application/octet-stream]\n",
            "Saving to: ‘eng.train.1’\n",
            "\n",
            "\reng.train.1           0%[                    ]       0  --.-KB/s               \reng.train.1         100%[===================>]   3.06M  --.-KB/s    in 0.07s   \n",
            "\n",
            "2024-03-21 03:24:02 (45.4 MB/s) - ‘eng.train.1’ saved [3213441/3213441]\n",
            "\n",
            "--2024-03-21 03:24:02--  https://princeton-nlp.github.io/cos484/assignments/a2/eng.val\n",
            "Resolving princeton-nlp.github.io (princeton-nlp.github.io)... 185.199.111.153, 185.199.109.153, 185.199.110.153, ...\n",
            "Connecting to princeton-nlp.github.io (princeton-nlp.github.io)|185.199.111.153|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 774436 (756K) [application/octet-stream]\n",
            "Saving to: ‘eng.val.1’\n",
            "\n",
            "eng.val.1           100%[===================>] 756.29K  --.-KB/s    in 0.05s   \n",
            "\n",
            "2024-03-21 03:24:02 (16.3 MB/s) - ‘eng.val.1’ saved [774436/774436]\n",
            "\n",
            "Found 20101 unique words (203621 in total)\n",
            "Found 5 unique named entity tags\n"
          ]
        }
      ],
      "source": [
        "# Download the dataset\n",
        "!wget https://princeton-nlp.github.io/cos484/assignments/a2/eng.train\n",
        "!wget https://princeton-nlp.github.io/cos484/assignments/a2/eng.val\n",
        "\n",
        "train_file = 'eng.train'\n",
        "test_file = 'eng.val'\n",
        "\n",
        "# Load the training data\n",
        "train_sentences = load_sentences(train_file)\n",
        "train_corpus, dic = prepare_dataset(train_sentences, mode='train', word_to_id=None, tag_to_id=None)\n",
        "\n",
        "# Load the testing data\n",
        "test_sentences = load_sentences(test_file)\n",
        "test_corpus = prepare_dataset(test_sentences, mode='test', word_to_id=dic['word_to_id'], tag_to_id=dic['tag_to_id'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_lines_to_display = 5\n",
        "\n",
        "# Read the first few lines of the training file\n",
        "print(\"Training Sample:\")\n",
        "with open(train_file, 'r', encoding='utf-8') as file:\n",
        "    for _ in range(num_lines_to_display):\n",
        "        line = file.readline().strip()\n",
        "        if line:\n",
        "            print(line)\n",
        "\n",
        "# Read the first few lines of the test file\n",
        "print(\"Test Sample:\")\n",
        "with open(test_file, 'r', encoding='utf-8') as file:\n",
        "    for _ in range(num_lines_to_display):\n",
        "        line = file.readline().strip()\n",
        "        if line:\n",
        "            print(line)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "caslRi1QQk68",
        "outputId": "ed8b39f9-10ff-4e67-f3b8-bd73cfd49bed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Sample:\n",
            "EU NNP I-NP ORG\n",
            "rejects VBZ I-VP O\n",
            "German JJ I-NP MISC\n",
            "call NN I-NP O\n",
            "to TO I-VP O\n",
            "Test Sample:\n",
            "CRICKET NNP I-NP O\n",
            "- : O O\n",
            "LEICESTERSHIRE NNP I-NP ORG\n",
            "TAKE NNP I-NP O\n",
            "OVER IN I-PP O\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HzNnUiBZS4ME"
      },
      "source": [
        "#### Experiment with an HMM with greedy decoding for Problem 2(a)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h0t7W9JMTfLl",
        "vscode": {
          "languageId": "python"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8999f529-6812-47b0-a779-971aef5c81f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train results:\n",
            "Tagging...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 14041/14041 [00:02<00:00, 5840.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---- 14041 lines tagged in 2.4106s ----\n",
            "Overall accuracy: 0.9544742438157164\n",
            "\n",
            "Tags:  {0: 'O', 1: 'PER', 2: 'ORG', 3: 'LOC', 4: 'MISC'}\n",
            "Confusion Matrix:\n",
            " [[168060    135   1233     48    102]\n",
            " [  1999   8628    456     39      6]\n",
            " [  1632     98   7291    731    273]\n",
            " [   741     49    567   6886     54]\n",
            " [   665     32    206    204   3486]]\n",
            "F-1 scores:  [0.98087109 0.85979073 0.73728385 0.84986115 0.81888654]\n",
            "\n",
            "-----------\n",
            "Test results:\n",
            "Tagging...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3490/3490 [00:00<00:00, 6123.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---- 3490 lines tagged in 0.5792s ----\n",
            "Overall accuracy: 0.9241331540561464\n",
            "\n",
            "Tags:  {0: 'O', 1: 'PER', 2: 'ORG', 3: 'LOC', 4: 'MISC'}\n",
            "Confusion Matrix:\n",
            " [[40558    17   543    17    29]\n",
            " [ 1105  1294   267    16     8]\n",
            " [  606    30  1382   176    56]\n",
            " [  249    15   215  1478    18]\n",
            " [  233     8    79    37   650]]\n",
            "F-1 scores:  [0.96664482 0.63838185 0.58361486 0.7991349  0.73529412]\n"
          ]
        }
      ],
      "source": [
        "runHiddenMarkovModel(\n",
        "    train_corpus = train_corpus,\n",
        "    test_corpus = test_corpus,\n",
        "    dic = dic,\n",
        "    decode_type = 'greedy',\n",
        "    output_file = None\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bP4iMShYX3Xp"
      },
      "source": [
        "#### Experiment with an HMM with viterbi decoding for Problem 2(b)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_zBEh-TpX8KC",
        "vscode": {
          "languageId": "python"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6272dab6-a4b8-484f-a8b6-00acb07eb9ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train results:\n",
            "Tagging...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/14041 [00:00<?, ?it/s]<ipython-input-13-a1eb62f844e3>:132: RuntimeWarning: divide by zero encountered in log\n",
            "  scores = M[i-1] + np.log(A[:, j]) + np.log(O[j, sentence[i]])\n",
            "100%|██████████| 14041/14041 [00:16<00:00, 846.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---- 14041 lines tagged in 16.6008s ----\n",
            "Overall accuracy: 0.9362050083242888\n",
            "\n",
            "Tags:  {0: 'O', 1: 'PER', 2: 'ORG', 3: 'LOC', 4: 'MISC'}\n",
            "Confusion Matrix:\n",
            " [[165769   2279    881    284    365]\n",
            " [  1845   9188     63     26      6]\n",
            " [  2493    602   6464    350    116]\n",
            " [  1979    153    396   5715     54]\n",
            " [   824    133     84     57   3495]]\n",
            "F-1 scores:  [0.96802808 0.78252353 0.72171049 0.7760201  0.8100591 ]\n",
            "\n",
            "-----------\n",
            "Test results:\n",
            "Tagging...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/3490 [00:00<?, ?it/s]<ipython-input-13-a1eb62f844e3>:132: RuntimeWarning: divide by zero encountered in log\n",
            "  scores = M[i-1] + np.log(A[:, j]) + np.log(O[j, sentence[i]])\n",
            "100%|██████████| 3490/3490 [00:03<00:00, 916.08it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---- 3490 lines tagged in 3.8198s ----\n",
            "Overall accuracy: 0.9110133235545776\n",
            "\n",
            "Tags:  {0: 'O', 1: 'PER', 2: 'ORG', 3: 'LOC', 4: 'MISC'}\n",
            "Confusion Matrix:\n",
            " [[39818   985   193    62   106]\n",
            " [  716  1918    38    10     8]\n",
            " [  813   177  1107   109    44]\n",
            " [  530    74   102  1246    23]\n",
            " [  264    66    36    12   629]]\n",
            "F-1 scores:  [0.95595703 0.64906937 0.5942029  0.72993556 0.69235003]\n"
          ]
        }
      ],
      "source": [
        "runHiddenMarkovModel(\n",
        "    train_corpus = train_corpus,\n",
        "    test_corpus = test_corpus,\n",
        "    dic = dic,\n",
        "    decode_type = 'viterbi',\n",
        "    output_file = None\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
